#!/usr/bin/env python3
import os
import json
import logging
from datetime import datetime, timedelta
from concurrent.futures import ProcessPoolExecutor
from typing import Dict
import string

import pandas as pd
import numpy as np
import pyarrow.dataset as ds
import yfinance as yf

# ---------------------------
# Configurable Parameters
# ---------------------------
DROP_TARGET        = 10.0       # % drop from EMA%FS to trigger a BUY
RISE_TARGET        = 10.0       # % rise from EMA%FS to trigger a SELL
EMA_INIT_INDEX     = 369        # use a 371-day EMA
TRADE_START_INDEX  = EMA_INIT_INDEX  # only start computing signals at this index
INVESTMENT_CAPITAL = 100
PROFIT_TARGET      = 0.30       # unused here, kept for backward compatibility
Strategy           = 1          # 1: buy signals only; 0: sell only; other: both
TARGET_ASSET       = ""         # e.g. "QQQ"; leave empty to process all
DELETE_ASSETS_FILE = None       # filled in main()

# smoothing constant (standard: 2/(span+1))
MULTIPLIER = 2 / (EMA_INIT_INDEX + 1)

# at top of script
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s  %(levelname)-8s  %(message)s",
)


# ---------------------------
# Utility: Delete-assets file
# ---------------------------
def add_asset_to_delete_file(symbol, name, delete_file_path):
    if not delete_file_path:
        return
    existing = set()
    if os.path.exists(delete_file_path):
        try:
            df0 = pd.read_csv(delete_file_path)
            existing |= set(df0["Symbol"].astype(str).str.strip())
        except Exception:
            pass
    if symbol not in existing:
        write_header = not os.path.exists(delete_file_path)
        with open(delete_file_path, "a") as f:
            if write_header:
                f.write("Symbol,Name\n")
            f.write(f"{symbol},{name}\n")


# ---------------------------
# Phase 1: Download & Merge Historical Data
# ---------------------------


def download_asset_data(asset_list_path, all_data_csv_file, all_data_parquet_file, delete_file_path):
    today = datetime.now().date()

    # ─── Quick‐exit if Parquet was updated today ──────────────────────────────────
    if os.path.exists(all_data_parquet_file):
        last_mod_date = datetime.fromtimestamp(
            os.path.getmtime(all_data_parquet_file)
        ).date()
        if last_mod_date == today:
            print("✅ Parquet data already updated today; skipping download.")
            # load and return the existing file
            df = pd.read_parquet(all_data_parquet_file)
            # make sure Date is normalized
            if "Date" in df.columns:
                df["Date"] = pd.to_datetime(df["Date"]).dt.strftime("%Y-%m-%d")
            return df

    # ─── Otherwise fall through into your existing download & merge logic ───────
    script_dir = os.path.dirname(os.path.abspath(__file__))
    # … rest of your Phase 1 code …
    # 1) Read Asset List
    try:
        df_assets = pd.read_csv(asset_list_path)
    except Exception as e:
        print(f"[ERROR] reading asset list: {e}")
        exit(1)

    # 2) Remove delete-listed symbols from asset list
    if os.path.exists(delete_file_path):
        try:
            del_df = pd.read_csv(delete_file_path)
            to_delete = set(del_df["Symbol"].astype(str).str.strip())
            before = len(df_assets)
            df_assets = df_assets[~df_assets["Symbol"].isin(to_delete)]
            after  = len(df_assets)
            if before != after:
                print(f"Removed {before-after} symbols from asset list per delete-file.")
                df_assets.to_csv(asset_list_path, index=False)
        except Exception as e:
            print(f"[WARN] updating asset list: {e}")

    unique = df_assets.drop_duplicates(subset=["Symbol","Name"])
    symbol_to_name = dict(zip(unique.Symbol, unique.Name))
    symbols = list(symbol_to_name.keys())

    today = datetime.now().date()
    end_fetch = today + timedelta(days=1)

    # 3) Figure out how far back to fetch
    if os.path.exists(all_data_parquet_file):
        mod_t = os.path.getmtime(all_data_parquet_file)
        last_date = datetime.fromtimestamp(mod_t).date()
        print(f"{all_data_parquet_file!r} last modified {last_date}")
        start_date = last_date + timedelta(days=1)
        if last_date == today:
            # re-fetch last 10 days to catch late fills
            start_date = today - timedelta(days=9)
            print("Re-fetching last 10 days due to recent update.")
    else:
        start_date = None
        print(f"No existing parquet; fetching full history.")

        # ─── 4) Download asset data using yfinance ────────────────────────────────────
    if TARGET_ASSET:
        # Skip any date checks and just pull full “max” history for the single symbol
        print(f"No existing Parquet check for TARGET_ASSET; fetching full history for {TARGET_ASSET}…")
        data = yf.download(
            symbols,
            period="max",
            interval="1d",
            group_by="column",
            auto_adjust=False,
            progress=True,
 	    threads=False
        )
    else:
        # Determine start date if Parquet exists
        if os.path.exists(all_data_parquet_file):
            mod_time      = os.path.getmtime(all_data_parquet_file)
            last_mod_date = datetime.fromtimestamp(mod_time).date()
            print(f"Assets_Data.parquet last modified on {last_mod_date}.")
            start_fetch_date = last_mod_date + timedelta(days=1)
            if last_mod_date == today:
                print(
                    f"{all_data_parquet_file!r} was last updated today "
                    f"({last_mod_date}); fetching last 10 days anyway."
                )
                start_fetch_date = today - timedelta(days=9)
        else:
            start_fetch_date = None
            print("'Assets_Data.parquet' does not exist. Fetching maximum data.")

        # Fetch either the last 10 days or the exact missing window
        if start_fetch_date:
            days_since = (today - start_fetch_date).days
            print(f"Existing Parquet last modified {days_since} day{'s' if days_since != 1 else ''} ago.")
            if days_since <= 10:
                print("Fetching last 10 days to grab any late fills and the latest close…")
                data = yf.download(
                    symbols,
                    period="10d",
                    interval="1d",
                    group_by="column",
                    auto_adjust=False,
                    progress=True,
                    threads=False
                )
            else:
                start_str = start_fetch_date.strftime("%Y-%m-%d")
                end_str   = end_fetch.strftime("%Y-%m-%d")
                print(f"Fetching missing window {start_str} → {end_str}…")
                data = yf.download(
                    symbols,
                    start=start_str,
                    end=end_str,
                    interval="1d",
                    group_by="column",
                    auto_adjust=False,
                    progress=True,
	  	    threads=False
                )
        else:
            print("No existing Parquet; fetching full history (period='max')…")
            data = yf.download(
                symbols,
                period="max",
                interval="1d",
                group_by="column",
                auto_adjust=False,
                progress=True,
                threads=False
            )

    new_df = process_downloaded_data(data, symbols, symbol_to_name)


       # 5) Read existing parquet (or init empty)
    print("\n=== Phase 1.1: Loading existing Parquet data ===")
    if os.path.exists(all_data_parquet_file):
        print(f"→ Found Parquet at: {all_data_parquet_file}")
        try:
            if TARGET_ASSET:
                ds0    = ds.dataset(all_data_parquet_file, format="parquet")
                tbl    =        ds0.to_table(filter=(ds.field("Symbol")==TARGET_ASSET))
                old_df = tbl.to_pandas()
            else:
                old_df = pd.read_parquet(all_data_parquet_file)
            if "Date" in old_df.columns:
                old_df["Date"] = pd.to_datetime(old_df["Date"]).dt.strftime("%Y-%m-%d")
        except Exception as e:
            print(f"[WARN] reading existing parquet: {e}")
            old_df = pd.DataFrame(columns=[
                "Symbol","Name","Date","Open","High","Low","Close"
            ])
    else:
        old_df = pd.DataFrame(columns=[
            "Symbol","Name","Date","Open","High","Low","Close"
        ])


    # 6) Exclude any delete-listed rows
    if os.path.exists(delete_file_path):
        try:
            del_df = pd.read_csv(delete_file_path)
            to_delete = set(del_df["Symbol"].astype(str).str.strip())
            before = len(old_df)
            old_df = old_df[~old_df["Symbol"].isin(to_delete)]
            after  = len(old_df)
            if before != after:
                print(f"Excluded {before-after} rows from existing data per delete file.")
            # clear delete file (keep header)
            with open(delete_file_path,"w") as f:
                f.write("Symbol,Name\n")
        except Exception as e:
            print(f"[WARN] processing delete list: {e}")

    # 7) Combine & write
    if not new_df.empty:
        combined = pd.concat([old_df, new_df], ignore_index=True)
        combined.drop_duplicates(subset=["Symbol","Date"], keep="last", inplace=True)
        combined.sort_values(["Symbol","Date"], inplace=True)

        try:
            combined.to_csv(all_data_csv_file, index=False, date_format="%Y-%m-%d")
            print(f"Wrote CSV: {all_data_csv_file}")
        except Exception as e:
            print(f"[WARN] saving CSV: {e}")

        try:
            combined.to_parquet(all_data_parquet_file, index=False)
            print(f"Wrote Parquet: {all_data_parquet_file}")
        except Exception as e:
            print(f"[WARN] saving Parquet: {e}")
    else:
        print("No new data; skipping write.")
        combined = old_df

    return combined




def process_downloaded_data(data, symbols, symbol_to_name):
    price_cols = ["Open","High","Low","Close"]
    frames = []

    for sym in symbols:
        try:
            if isinstance(data.columns, pd.MultiIndex):
                if sym not in data.columns.levels[1]:
                    continue
                df0 = data.xs(sym, axis=1, level=1).reset_index()
            else:
                df0 = data.reset_index()
            df0.dropna(subset=price_cols, inplace=True)
            if df0.empty:
                continue
            df0["Date"] = pd.to_datetime(df0["Date"]).dt.strftime("%Y-%m-%d")
            df0[price_cols] = df0[price_cols].round(2)
            df0["Symbol"] = sym
            df0["Name"]   = symbol_to_name.get(sym, "")
            frames.append(df0[["Symbol","Name","Date"] + price_cols])
        except Exception:
            continue

    if frames:
        return pd.concat(frames, ignore_index=True)
    else:
        return pd.DataFrame(columns=["Symbol","Name","Date","Open","High","Low","Close"])





# ──────────────────────────────────────────────────────────────────────────
# ────────────────────────────────────────────────────────
# Phase 2.0 Helper: “Three-band” Entry Logic
#   (green / gold / blue), exactly as in UBC Processor.php
# ────────────────────────────────────────────────────────
def calculate_max_drop_forward(df: pd.DataFrame, idx: int, days: int) -> float:
    entry = df.at[idx, "Low"]
    window = df["Low"].iloc[idx+1 : idx+1+days]
    if window.empty or pd.isna(entry) or entry == 0:
        return None
    min_low = window.min()
    pct = round(100 * (entry - min_low) / entry, 2)
    return pct if pct > 0 else 0.0

def calculate_max_rise_forward(df: pd.DataFrame, idx: int, days: int) -> float:
    entry = df.at[idx, "Close"]
    window = df["High"].iloc[idx+1 : idx+1+days]
    if window.empty or pd.isna(entry) or entry == 0:
        return None
    max_high = window.max()
    pct = round(100 * (max_high - entry) / entry, 2)
    return pct if pct > 0 else 0.0





def compute_phase2_entries(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy().reset_index(drop=True)
    n  = len(df)

    # 2.0) clear any existing entry fields
    for col in (
        "entryColor","EntryPrice","EntryClose","DrawdownFromATH",
        "DipFromATH","MaxDrop30","MaxDrop90","MaxDrop3Y",
        "MaxRise1Y","MaxRise3Y","Baseline","Baseline2"
    ):
        df[col] = None

    # 2.1) last valid EMA%FS baseline
    last_valid = None
    if "EMA%FS" in df:
        for i in range(n-1, -1, -1):
            v = df.at[i, "EMA%FS"]
            if pd.notna(v):
                last_valid = v
                break

    # 2.1a) global ATH & current close
    ath = df["High"].max() if not df["High"].empty else None
    current_close = df["Close"].iat[-1] if not df["Close"].empty else None

    # 2.1b) DipFromATH & Baseline2 per row
    highs = df["High"]
    for i in range(n):
        slice_highs = highs.iloc[: i+1]
        ath_so_far  = slice_highs.max() if not slice_highs.empty else None
        low         = df.at[i, "Low"]
        if pd.notna(low) and ath_so_far and ath_so_far != 0:
            df.at[i, "DipFromATH"] = round(100 * (ath_so_far - low) / ath_so_far, 2)
        if current_close and ath and ath != 0:
            df.at[i, "Baseline2"] = round(100 * (ath - current_close) / ath, 2)

    # 2.1c) today's dip
    current_dip_pct = round(100 * (ath - current_close) / ath, 2) if ath and current_close else None

    # 2.2) first green & first gold
    await_green = await_gold = False
    first_green = first_gold = None
    tolerance   = 0.25

    for i in range(n):
        df.at[i, "Baseline"]  = last_valid
        df.at[i, "Baseline2"] = current_dip_pct

        if i < EMA_INIT_INDEX or pd.isna(df.at[i, "EMA%FS"]):
            continue

        action = df.at[i, "Action"]
        if action == "Start Tracking":
            await_green = await_gold = True

        # green
        if await_green and last_valid is not None:
            pctfs = df.at[i, "EMA%FS"]
            if (last_valid < 0 and pctfs <= last_valid) or (last_valid >= 0 and pctfs >= last_valid):
                df.at[i, "entryColor"]     = "green"
                df.at[i, "EntryPrice"]     = df.at[i, "Low"]
                df.at[i, "EntryClose"]     = df.at[i, "Close"]
                df.at[i, "DrawdownFromATH"]= df.at[i, "DipFromATH"]
                # forward stats:
                df.at[i, "MaxDrop30"]      = calculate_max_drop_forward(df, i, 30)
                df.at[i, "MaxDrop90"]      = calculate_max_drop_forward(df, i, 90)
                df.at[i, "MaxDrop3Y"]      = calculate_max_drop_forward(df, i, 365)
                df.at[i, "MaxRise1Y"]      = calculate_max_rise_forward(df, i, 365)
                df.at[i, "MaxRise3Y"]      = calculate_max_rise_forward(df, i, 365)
                first_green = i
                await_green = False

        # gold
        if await_gold and pd.notna(df.at[i, "DipFromATH"]) and pd.notna(df.at[i, "Baseline2"]):
            if abs(df.at[i, "DipFromATH"] - df.at[i, "Baseline2"]) <= tolerance:
                df.at[i, "entryColor"]     = "gold"
                df.at[i, "EntryPrice"]     = df.at[i, "Low"]
                df.at[i, "EntryClose"]     = df.at[i, "Close"]
                df.at[i, "DrawdownFromATH"]= df.at[i, "DipFromATH"]
                df.at[i, "MaxDrop30"]      = calculate_max_drop_forward(df, i, 30)
                df.at[i, "MaxDrop90"]      = calculate_max_drop_forward(df, i, 90)
                df.at[i, "MaxDrop3Y"]      = calculate_max_drop_forward(df, i, 365)
                df.at[i, "MaxRise1Y"]      = calculate_max_rise_forward(df, i, 365)
                df.at[i, "MaxRise3Y"]      = calculate_max_rise_forward(df, i, 365)
                first_gold  = i
                await_gold  = False

    # 2.3) second pass for blue (±5% of current close)
    blue_low  = current_close * 0.95 if current_close else None
    blue_high = current_close * 1.05 if current_close else None
    await_blue = False
    first_blue = None

    for i in range(n):
        if df.at[i, "Action"] == "Start Tracking":
            await_blue = True

        low = df.at[i, "Low"]
        if await_blue and blue_low and blue_high and low >= blue_low and low <= blue_high:
            df.at[i, "entryColor"]     = "blue"
            df.at[i, "EntryPrice"]     = low
            df.at[i, "EntryClose"]     = df.at[i, "Close"]
            df.at[i, "DrawdownFromATH"]= df.at[i, "DipFromATH"]
            df.at[i, "MaxDrop30"]      = calculate_max_drop_forward(df, i, 30)
            df.at[i, "MaxDrop90"]      = calculate_max_drop_forward(df, i, 90)
            df.at[i, "MaxDrop3Y"]      = calculate_max_drop_forward(df, i, 365)
            df.at[i, "MaxRise1Y"]      = calculate_max_rise_forward(df, i, 365)
            df.at[i, "MaxRise3Y"]      = calculate_max_rise_forward(df, i, 365)
            first_blue = i
            await_blue = False

        # preserve green if not overwritten
        if first_green is not None and i == first_green:
            df.at[i, "entryColor"] = "green"

    return df

# ──────────────────────────────────────────────────────────────────────────
# Phase 2: Per-Asset Indicator Logic
# ──────────────────────────────────────────────────────────────────────────
def add_tracking_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    Phase 2.0: Compute EMA, WasAboveEMA, Action, LastEMA, EMA%FS columns exactly like UBC Processor.php,
    with baseline reset on Stop Tracking so each new segment gets a fresh EMA%FS baseline.
    """
    df = df.copy().reset_index(drop=True)
    df["EMA"] = None
    df["WasAboveEMA"] = None
    df["Action"] = None
    df["LastEMA"] = None
    df["EMA%FS"] = None

    n = len(df)
    if n < EMA_INIT_INDEX:
        return df

    # Initial EMA = average of first EMA_INIT_INDEX highs
    ema = df["High"].iloc[:EMA_INIT_INDEX].mean()
    baseline_value = None

    for i in range(EMA_INIT_INDEX, n):
        high = df.at[i, "High"]
        low  = df.at[i, "Low"]

        # update EMA
        ema = round(high * MULTIPLIER + ema * (1 - MULTIPLIER), 2)
        df.at[i, "EMA"] = ema

        # flag if price above EMA
        was_above = high > ema
        df.at[i, "WasAboveEMA"] = was_above

        # determine Action and reset baseline on Stop Tracking
        if i == EMA_INIT_INDEX:
            action = "Stop Tracking" if was_above else "Start Tracking"
        else:
            prev = df.at[i-1, "Action"]
            if was_above:
                action = "Stop Tracking"
                baseline_value = None            # ← reset baseline when breaking above EMA
            elif prev in ("Start Tracking", "Keep Tracking"):
                action = "Keep Tracking"
            else:
                action = "Start Tracking"

        df.at[i, "Action"] = action

        # record LastEMA only when starting a segment
        df.at[i, "LastEMA"] = ema if action == "Start Tracking" else None

        # set baseline on the first Start Tracking of this segment
        if action == "Start Tracking" and baseline_value is None:
            baseline_value = ema

        # compute EMA%FS when a baseline is set
        if baseline_value and baseline_value != 0:
            df.at[i, "EMA%FS"] = round(100 * (low - baseline_value) / baseline_value, 2)
        else:
            df.at[i, "EMA%FS"] = None

    return df







def compute_three_band_entries(df: pd.DataFrame) -> pd.DataFrame:
    """
    Phase 2: Implements the three-band entry logic (green, gold, blue)
    and calculates forward-looking max drop and rise stats.
    """
    tracker = df.copy().reset_index(drop=True)

    # 2.0) Clear old entry-related columns
    old_cols = [
        "entryColor", "EntryPrice", "EntryClose",
        "DrawdownFromATH", "DipFromATH",
        "MaxDrop30", "MaxDrop90", "MaxDrop3Y",
        "MaxRise1Y", "MaxRise3Y",
        "Baseline", "Baseline2"
    ]
    for col in old_cols:
        if col in tracker.columns:
            tracker.drop(columns=col, inplace=True)

    # Initialize 'entryColor' with dtype object (to avoid dtype warnings)
    tracker["entryColor"] = pd.Series([None] * len(tracker), dtype="object")

    # 2.1) Find last valid EMA%FS value (baseline for green entries)
    last_valid = None
    for val in tracker["EMA%FS"].dropna()[::-1]:
        last_valid = val
        break

    # 2.1a) Global All-Time High (ATH) and most recent close
    ath = tracker["High"].max() if not tracker.empty else None
    current_close = tracker["Close"].iat[-1] if not tracker.empty else None

    # 2.1b) Calculate DipFromATH (running max of highs) and Baseline2 per row
    for i in range(len(tracker)):
        highs_so_far = tracker["High"].iloc[:i + 1]
        ath_so_far = highs_so_far.max() if not highs_so_far.empty else None
        close_val = tracker.at[i, "Close"]
        if pd.notna(close_val) and ath_so_far and ath_so_far != 0:
            tracker.at[i, "DipFromATH"] = round(100 * (ath_so_far - close_val) / ath_so_far, 2)
        if current_close and ath and ath != 0:
            tracker.at[i, "Baseline2"] = round(100 * (ath - current_close) / ath, 2)

    # 2.1c) Today's drawdown percentage (Baseline2 at last row)
    current_dip_pct = round(100 * (ath - current_close) / ath, 2) if ath and current_close else None

    # 2.2) First pass: find first green and gold entries per segment
    await_green = False
    await_gold = False
    first_green = None
    first_gold = None
    tolerance = 0.25

    for i in range(len(tracker)):
        tracker.at[i, "Baseline"] = last_valid
        tracker.at[i, "Baseline2"] = current_dip_pct

        # Skip rows before EMA_INIT_INDEX or missing EMA%FS
        if i < EMA_INIT_INDEX or pd.isna(tracker.at[i, "EMA%FS"]):
            continue

        if tracker.at[i, "Action"] == "Start Tracking":
            await_green = True
            await_gold = True

        # Green entry condition
        fs = tracker.at[i, "EMA%FS"]
        if await_green and last_valid is not None and (
            (last_valid < 0 and fs <= last_valid) or (last_valid >= 0 and fs >= last_valid)
        ):
            tracker.at[i, "entryColor"] = "green"
            tracker.at[i, "EntryPrice"] = tracker.at[i, "Low"]
            tracker.at[i, "EntryClose"] = tracker.at[i, "Close"]
            tracker.at[i, "DrawdownFromATH"] = tracker.at[i, "DipFromATH"]
            first_green = i
            await_green = False

        # Gold entry condition (DipFromATH near Baseline2 within tolerance)
        dip = tracker.at[i, "DipFromATH"]
        base2 = tracker.at[i, "Baseline2"]
        if await_gold and pd.notna(dip) and pd.notna(base2) and abs(dip - base2) <= tolerance:
            tracker.at[i, "entryColor"] = "gold"
            tracker.at[i, "EntryPrice"] = tracker.at[i, "Low"]
            tracker.at[i, "EntryClose"] = tracker.at[i, "Close"]
            tracker.at[i, "DrawdownFromATH"] = dip
            first_gold = i
            await_gold = False

    # 2.3) Blue band boundaries ±5% of current close
    blue_lower = current_close * 0.95 if current_close else None
    blue_upper = current_close * 1.05 if current_close else None

    # 2.4) Second pass: tag gold and blue entries per segment
    await_gold = False
    await_blue = False
    first_gold = None
    first_blue = None

    for i in range(len(tracker)):
        if tracker.at[i, "Action"] == "Start Tracking":
            await_gold = True
            await_blue = True

        # Gold tagging (again)
        dip = tracker.at[i, "DipFromATH"]
        base2 = tracker.at[i, "Baseline2"]
        if await_gold and pd.notna(dip) and pd.notna(base2) and abs(dip - base2) <= tolerance:
            tracker.at[i, "entryColor"] = "gold"
            tracker.at[i, "EntryPrice"] = tracker.at[i, "Low"]
            tracker.at[i, "EntryClose"] = tracker.at[i, "Close"]
            tracker.at[i, "DrawdownFromATH"] = dip
            first_gold = i
            await_gold = False

        # Blue tagging: price within ±5% band of current close
        low_val = tracker.at[i, "Low"]
        if await_blue and blue_lower is not None and pd.notna(low_val) and blue_lower <= low_val <= blue_upper:
            tracker.at[i, "entryColor"] = "blue"
            tracker.at[i, "EntryPrice"] = low_val
            tracker.at[i, "EntryClose"] = tracker.at[i, "Close"]
            tracker.at[i, "DrawdownFromATH"] = tracker.at[i, "DipFromATH"]
            first_blue = i
            await_blue = False

        # Preserve green if not overwritten by gold/blue
        if i == first_green and (pd.isna(tracker.at[i, "entryColor"]) or tracker.at[i, "entryColor"] == "green"):
            tracker.at[i, "entryColor"] = "green"

    # 2.5) Forward-looking max drop and rise stats functions
    def fwd_stats(col_name, func, days):
        tracker[col_name] = np.nan
        n = len(tracker)
        for i in range(EMA_INIT_INDEX, n):
            if pd.notna(tracker.at[i, "entryColor"]):
                tracker.at[i, col_name] = func(i, days, tracker)

    def calc_max_drop(idx, days, dfk):
        lows = np.array(dfk["Low"].iloc[idx + 1 : idx + 1 + days])
        if lows.size:
            low0 = dfk.at[idx, "Low"]
            return round(100 * (low0 - lows.min()) / low0, 2) if low0 else None
        return None

    def calc_max_rise(idx, days, dfk):
        highs = np.array(dfk["High"].iloc[idx + 1 : idx + 1 + days])
        if highs.size:
            h0 = dfk.at[idx, "Close"]
            return round(100 * (highs.max() - h0) / h0, 2) if h0 else None
        return None

    # Calculate forward stats
    fwd_stats("MaxDrop30", calc_max_drop, 30)
    fwd_stats("MaxRise1Y", calc_max_rise, 365)
    fwd_stats("MaxDrop90", calc_max_drop, 90)
    fwd_stats("MaxRise3Y", calc_max_rise, 1095)
    fwd_stats("MaxDrop3Y", calc_max_drop, 1095)

    return tracker


import pandas as pd

def is_bad_last_bar(df: pd.DataFrame) -> bool:
    """
    Returns True if the final row’s Open/High/Low/Close
    contains any zero or if all four values are exactly the same.
    """
    if df.empty:
        return False

    o, h, l, c = df.iloc[-1][["Open","High","Low","Close"]].values
    # any zero
    if 0 in (o, h, l, c):
        return True
    # all the same
    if o == h == l == c:
        return True
    return False


def get_current_baseline(df: pd.DataFrame) -> float:
    """
    Returns the last non-null EMA%FS value from df or None.
    """
    vals = df["EMA%FS"].dropna()
    return float(vals.iat[-1]) if not vals.empty else None

def process_asset_group(group: pd.DataFrame) -> pd.DataFrame:
    symbol = group["Symbol"].iat[0] if not group.empty else "(unknown)"

    # ── Pre-filter: drop any non-final rows where any OHLC equals zero ───
    df_clean = group.copy().reset_index(drop=True)
    if not df_clean.empty:
        last_idx = len(df_clean) - 1
        mask = ~((df_clean[['Open','High','Low','Close']] == 0).any(axis=1) & (df_clean.index != last_idx))
        df_clean = df_clean.loc[mask].reset_index(drop=True)
    group = df_clean

    # ── Pre-filter: remove all rows prior to a >80% change between consecutive closes ───
    if len(group) > 1:
        pct_change = (group['Close'].pct_change().abs() * 100)
        jump_idx = pct_change[pct_change > 80].first_valid_index()
        if jump_idx and jump_idx > 0:
            group = group.iloc[jump_idx:].reset_index(drop=True)

    # ── First check: skip if last OHLC bar is zero or flat ───────────────────

    # ── Ensure enough data points ────────────────────────────────────────────
    if group.empty or len(group) < EMA_INIT_INDEX:
        print(f"Skipping {symbol}: not enough data points (have {len(group)}, need {EMA_INIT_INDEX})")
        return pd.DataFrame()

    # ── Skip if last OHLC bar is zero or flat ──────────────────────────────
    if is_bad_last_bar(group):
        print(f"Skipping {symbol}: last bar has zero or flat OHLC values")
        return pd.DataFrame()

    # ── Prepare data and compute indicators ─────────────────────────────────
    df = group.copy().reset_index(drop=True)
    df = add_tracking_columns(df)
    df = compute_three_band_entries(df)

    # ── Extract the most recent EMA%FS as baseline for strategy filtering ─────
    ema_fs_series = df["EMA%FS"].dropna()
    if ema_fs_series.empty:
        print(f"Skipping {symbol}: no EMA%FS values to use as baseline")
        return pd.DataFrame()
    baseline = ema_fs_series.iat[-1]
    
    # ── Apply strategy filter using the baseline ─────────────────────────────
    if Strategy == 1 and baseline >= -DROP_TARGET:
        print(f"Skipping {symbol}: Baseline EMA%FS = {baseline:.2f}% ≥ –{DROP_TARGET}%")
        return pd.DataFrame()
    if Strategy == 0 and baseline <= RISE_TARGET:
        print(f"Skipping {symbol}: Baseline EMA%FS = {baseline:.2f}% ≤ {RISE_TARGET}%")
        return pd.DataFrame()

    # ── Skip if no entryColor generated ────────────────────────────────────
    if "entryColor" not in df.columns or df["entryColor"].dropna().empty:
        print(f"Skipping {symbol}: no entry points generated")
        return pd.DataFrame()

    print(f"Including {symbol}: Baseline EMA%FS = {baseline:.2f}%")
    return df
    """
    Returns True if the final row’s Open/High/Low/Close
    contains any zero or if all four values are exactly the same.
    """
    if df.empty:
        return False

    o, h, l, c = df.iloc[-1][["Open","High","Low","Close"]].values
    # any zero
    if 0 in (o, h, l, c):
        return True
    # all the same
    if o == h == l == c:
        return True
    return False

def process_asset_group(group: pd.DataFrame) -> pd.DataFrame:
    symbol = group["Symbol"].iat[0] if not group.empty else "(unknown)"
    # ── First check: skip if last OHLC bar is zero or flat ───────────────────
    if is_bad_last_bar(group):
        print(f"Skipping {symbol}: last bar has zero or flat OHLC values")
        return pd.DataFrame()

    # ── Next: ensure enough data points ─────────────────────────────────────
    if group.empty or len(group) < EMA_INIT_INDEX:
        print(f"Skipping {symbol}: not enough data points (have {len(group)}, need {EMA_INIT_INDEX})")
        return pd.DataFrame()

    # Make a working copy
    df = group.copy().reset_index(drop=True)

    # Phase 2: compute tracking, three-band entries
    df = add_tracking_columns(df)
    df = compute_three_band_entries(df)

    # if no entryColor at all, skip
    if "entryColor" not in df.columns or df["entryColor"].dropna().empty:
        print(f"Skipping {symbol}: no entry points generated")
        return pd.DataFrame()

    # final strategy filter: grab last EMA%FS if it exists
    last_pct_series = df["EMA%FS"].dropna()
    if last_pct_series.empty:
        print(f"Skipping {symbol}: no EMA%FS values")
        return pd.DataFrame()
    last_pct = last_pct_series.iat[-1]

    if Strategy == 1 and last_pct >= -DROP_TARGET:
        print(f"Skipping {symbol}: Last EMA%FS = {last_pct:.2f}% ≥ –{DROP_TARGET}%")
        return pd.DataFrame()
    if Strategy == 0 and last_pct <= RISE_TARGET:
        print(f"Skipping {symbol}: Last EMA%FS = {last_pct:.2f}% ≤ {RISE_TARGET}%")
        return pd.DataFrame()

    print(f"Including {symbol}: Last EMA%FS = {last_pct:.2f}%")
    return df


    """
    Returns True if the final row’s Open/High/Low/Close
    contains any zero or if all four values are exactly the same.
    """
    if df.empty:
        return False

    o, h, l, c = df.iloc[-1][["Open","High","Low","Close"]].values
    # any zero
    if 0 in (o, h, l, c):
        return True
    # all the same
    if o == h == l == c:
        return True
    return False

def process_asset_group(group: pd.DataFrame) -> pd.DataFrame:
    symbol = group["Symbol"].iat[0] if not group.empty else "(unknown)"

    # ── First: skip if last OHLC bar is bad
    if is_bad_last_bar(group):
        print(f"Skipping {symbol}: last bar has zero or flat OHLC values")
        return pd.DataFrame()

    # ── Next: ensure enough data points
    if group.empty or len(group) < EMA_INIT_INDEX:
        print(f"Skipping {symbol}: not enough data points (have {len(group)}, need {EMA_INIT_INDEX})")
        return pd.DataFrame()

    # ── Compute indicators
    df = group.copy().reset_index(drop=True)
    df = add_tracking_columns(df)
    df = compute_three_band_entries(df)

    # ── Skip if the very last row has no EMA%FS
    last_ema_fs = df["EMA%FS"].iat[-1]
    if pd.isna(last_ema_fs):
        print(f"Skipping {symbol}: final row has no EMA%FS")
        return pd.DataFrame()

    # ── Use that last EMA%FS as baseline for strategy filter
    baseline = last_ema_fs
    if Strategy == 1 and baseline >= -DROP_TARGET:
        print(f"Skipping {symbol}: Baseline EMA%FS = {baseline:.2f}% ≥ –{DROP_TARGET}%")
        return pd.DataFrame()
    if Strategy == 0 and baseline <= RISE_TARGET:
        print(f"Skipping {symbol}: Baseline EMA%FS = {baseline:.2f}% ≤ {RISE_TARGET}%")
        return pd.DataFrame()

    # ── Finally, skip if no entryColor generated
    if "entryColor" not in df.columns or df["entryColor"].dropna().empty:
        print(f"Skipping {symbol}: no entry points generated")
        return pd.DataFrame()

    print(f"Including {symbol}: Baseline EMA%FS = {baseline:.2f}%")
    return df




# helper for ProcessPoolExecutor
def process_asset_group_wrapper(group):
    return process_asset_group(group)




# ---------------------------
# Phase 3a) Excel
# ---------------------------
    rpt_df = create_report_sheet(proc_df)

    # ── drop any HOLD signals ───────────────────────
    rpt_df = rpt_df[rpt_df["Signal"] != "HOLD"]
    if rpt_df.empty:
        print("No BUY/SELL signals; nothing to report. Exiting.")
        return

    with pd.ExcelWriter(excel_file, engine="xlsxwriter") as writer:
        rpt_df.to_excel(writer, sheet_name="Report", index=False)
        ...

# ──────────────────────────────────────────────────────────────────────────
# Phase 3b) Helpers & Report-building
# ──────────────────────────────────────────────────────────────────────────


def build_report_dict(df: pd.DataFrame) -> dict:
    """
    Build per-asset report dict in the original UBC JSON format.
    entriesData is a dict keyed by the original DataFrame index.
    Field names and casing exactly match the PHP version.
    """
    try:
        # ── Pull in the pre-computed baseline, else fall back ───────────────
        current_baseline = df.attrs.get("currentBaseline")
        if current_baseline is None:
            vals = df["EMA%FS"].dropna()
            current_baseline = round(vals.iat[-1], 2) if not vals.empty else None

        # 1) pick out only the rows with entryColor
        entries = df[df["entryColor"].notnull()].copy()

        


        # 2) rename columns to the exact PHP names / casing, including metadata
        cols_map = {
            "Symbol":         "Symbol",
            "Name":           "Name",
            "Open":           "Open",
            "Date":           "date",
            "Low":            "low",
            "High":           "high",
            "Close":          "close",
            "EMA":            "EMA",
            "WasAboveEMA":    "WasAboveEMA",
            "Action":         "Action",
            "LastEMA":        "LastEMA",
            "EMA%FS":         "EMA%FS",
            "DipFromATH":     "DipFromATH",
            "Baseline2":      "Baseline2",
            "Baseline":       "Baseline",
            "entryColor":     "entryColor",
            "EntryPrice":     "EntryPrice",
            "EntryClose":     "EntryClose",
            "DrawdownFromATH":"DrawdownFromATH",
            "MaxDrop30":      "MaxDrop30",
            "MaxDrop90":      "MaxDrop90",
            "MaxDrop3Y":      "MaxDrop3Y",
            "MaxRise1Y":      "MaxRise1Y",
            "MaxRise3Y":      "MaxRise3Y",
        }
        entries = entries.rename(columns=cols_map)

        # ensure every PHP field exists without overwriting real data
        for php_col in cols_map.values():
            if php_col not in entries.columns:
                entries[php_col] = None

        # 3) convert to dict keyed by original DataFrame index
        entries_data = entries.to_dict(orient="index")
        total_entries = len(entries_data)

        # 4) summary scalars from full df
        recent_close     = float(df["Close"].iat[-1]) if not df.empty else None
        ath              = float(df["High"].max())  if not df.empty else None
        current_dip_pct  = (
            round(100 * (ath - recent_close) / ath, 2)
            if ath and recent_close else None
       
        )
        last_entry_close = float(entries["EntryClose"].iat[-1]) if total_entries else None
        entry_dd         = round(entries["DrawdownFromATH"].iat[-1], 2) if total_entries else None

        # 5) interpretation logic
        def pct_max(col):
            vals = entries[col].dropna()
            return round(float(vals.max()), 2) if not vals.empty else 0.0

        def min_rise(cols):
            vals = pd.concat([entries[c].dropna() for c in cols], ignore_index=True)
            pos  = vals[vals > 0]
            return round(float(pos.min()), 2) if not pos.empty else 0.0

        interp = {
            "Risk (30 d)":   f"Max drawdown in 30 days was {pct_max('MaxDrop30')}%.",
            "Risk (90 d)":   f"Max drawdown in 90 days was {pct_max('MaxDrop90')}%.",
            "Risk (3 y)":    f"Max drawdown in 3 years was {pct_max('MaxDrop3Y')}%.",
            "Reward Upside": f"Min-case upside (1 y & 3 y excl. latest) was "
                             f"{min_rise(['MaxRise1Y','MaxRise3Y'])}%.",
        }
        for color in ("green","gold","blue"):
            for span, col in [("1 y","MaxRise1Y"), ("3 y","MaxRise3Y")]:
                vs = entries.loc[entries["entryColor"]==color, col].dropna()
                if not vs.empty:
                    p90 = round(float(np.percentile(vs, 90)), 2)
                    interp[f"{color.capitalize()} 90‰-tile {span} rise"] = (
                        f"🚦 {color.capitalize()}: 90th percentile {span} rise: {p90}%.")

        # 6) BUY/HOLD/SELL signal
        buy  = (min_rise(["MaxRise1Y","MaxRise3Y"]) >= RISE_TARGET) or any(
            entries[c].dropna().quantile(0.8) >= RISE_TARGET
            for c in ("MaxRise1Y","MaxRise3Y") if not entries[c].dropna().empty
        )
        sell = any(pct_max(c) >= DROP_TARGET for c in ("MaxDrop30","MaxDrop90","MaxDrop3Y"))
        if buy:
            interp["Signal"] = "🚀 BUY: upside conditions met."
        elif sell:
            interp["Signal"] = "🚨 SELL: drawdown conditions met."
        else:
            interp["Signal"] = "⚠️ HOLD: no clear signal."

        # 7) Profit target, remaining profit
        if last_entry_close is not None and recent_close is not None:
            if buy:
                profit_target = last_entry_close * 1.10
            else:
                worst = min_rise(['MaxRise1Y','MaxRise3Y'])
                profit_target = last_entry_close * (1 + worst/100)
            remaining_pct = f"{round(100*(profit_target - recent_close)/recent_close, 2)}%"
        else:
            profit_target, remaining_pct = None, 'N/A'

        interp['Entry Price']      = f"{last_entry_close:.2f}" if last_entry_close is not None else 'N/A'
        interp['Profit Target']    = f"{profit_target:.2f}"    if profit_target is not None else 'N/A'
        interp['Remaining Profit'] = remaining_pct

        # 8) Hit ratios for ≥ RISE_TARGET %
        entries_excl = entries.iloc[:-1] if total_entries > 1 else pd.DataFrame()
        def rise_stats(df, fld):
            vals = df[fld].dropna().astype(float)
            t = len(vals)
            h = (vals >= RISE_TARGET).sum()
            pct = round(100*h/t,1) if t else 0
            return h, t, f"{h}/{t} entries ≥ {RISE_TARGET}% ({pct}%)"
        hits = {}
        for fld, lbl in [("MaxRise1Y","1 Y"),("MaxRise3Y","3 Y")]:
            h,t,label = rise_stats(entries_excl, fld)
            hits[f"All {lbl}"] = f"✅ All {lbl}: {label}"
            for col in ("green","gold","blue"):
                sub = entries_excl[entries_excl["entryColor"]==col]
                h2,t2,label2 = rise_stats(sub, fld)
                hits[f"{col.capitalize()} {lbl}"] = f"🎯 {col.capitalize()} {lbl}: {label2}"
        interp['Hit Ratios'] = hits

        # 9) assemble and return
        return {
            "totalEntries":                total_entries,
            "entriesData":                 entries_data,
            "currentBaseline":             f"{current_baseline}%" if current_baseline is not None else None,
            "recentClose":                 recent_close,
            "lastEntryClose":              last_entry_close,
            "currentEntryDrawdownFromATH": f"{entry_dd}%" if entry_dd is not None else None,
            "currentDipFromATH":           f"{current_dip_pct}%" if current_dip_pct is not None else None,
            "interpretation":              interp
        }

    except Exception as e:
        logging.error(f"build_report_dict failed: {e}")
        return {
            "totalEntries": 0,
            "entriesData":  {},
            "currentBaseline": None,
            "recentClose": None,
            "lastEntryClose": None,
            "currentEntryDrawdownFromATH": None,
            "currentDipFromATH": None,
            "interpretation": {"Signal": "⚠️ HOLD: error generating report."}
        }






def create_report_sheet(
    processed_df: pd.DataFrame,
    interpretations: Dict[str, Dict[str, str]]
) -> pd.DataFrame:
    """
    Build the Excel “Report” sheet with:
      - Name, Symbol
      - Green EMA%FS  (as fraction)
      - Gold Dip      (as fraction)
      - Blue Price Action (as numeric)
      - Signal
      - % Drop30…% Rise365 (as fractions)
    Sorted ascending by Green EMA%FS.
    """
    rows = []
    

    for sym, grp in processed_df.groupby("Symbol"):
        grp = grp.sort_values("Date").reset_index(drop=True)
        last = grp.iloc[-1]
        side = "BUY" if last["Action"] in ("Start Tracking", "Keep Tracking") else "SELL"

        # hyperlinks
        name = grp.at[0, "Name"]
        name_link   = f'=HYPERLINK("#\'{sym}\'!A1", "{name}")'
        symbol_link = f'=HYPERLINK("https://finance.yahoo.com/chart/{sym}", "{sym}")'

        # pull out the report block
        interp = interpretations.get(sym, {})

        # parse Green EMA%FS → fraction
        gb = interp.get("currentBaseline", "")
        try:
            green_frac = float(gb.rstrip("%")) / 100
        except:
            green_frac = None

        # parse Gold Dip → fraction
        gd = interp.get("currentDipFromATH", "")
        try:
            gold_frac = float(gd.rstrip("%")) / 100
        except:
            gold_frac = None

        # Blue Price Action stays numeric
        blue_price = interp.get("recentClose", None)

        # build the multi-line Signal
        msgs = []
        for color in ("green", "gold", "blue"):
            sub = grp[grp["entryColor"] == color]
            if sub.empty:
                continue
            p1 = round(float(np.percentile(sub["MaxRise1Y"].dropna(), 90)), 2) if not sub["MaxRise1Y"].dropna().empty else None
            p3 = round(float(np.percentile(sub["MaxRise3Y"].dropna(), 90)), 2) if not sub["MaxRise3Y"].dropna().empty else None
            if p3 is not None:
                msgs.append(f"{side}: 90%-tile 3 y upside ({color}) ≥ {RISE_TARGET}% ({p3}%). 🚀")
            if p1 is not None:
                msgs.append(f"{side}: 90%-tile 1 y upside ({color}) ≥ {RISE_TARGET}% ({p1}%). 🚀")
        signal = "\n".join(msgs) if msgs else f"{side}: no data"

        # isolate only the entry rows for our six forward‐stats
        entries = grp[grp["entryColor"].notnull()]
        n = len(entries)
        def pct_entry(col, thr):
            return (entries[col].ge(thr).sum() / n) if n else 0.0

        rows.append({
            "Name":               name_link,
            "Symbol":             symbol_link,
            "Green EMA%FS":       green_frac,
            "Gold Dip":           gold_frac,
            "Blue Price Action":  blue_price,
            "Signal":             signal,
            "% Drop30":           pct_entry("MaxDrop30", DROP_TARGET),
            "% Rise30":           pct_entry("MaxRise1Y", RISE_TARGET),
            "% Drop90":           pct_entry("MaxDrop90", DROP_TARGET),
            "% Rise90":           pct_entry("MaxRise3Y", RISE_TARGET),
            "% Drop365":          pct_entry("MaxDrop3Y", DROP_TARGET),
            "% Rise365":          pct_entry("MaxRise3Y", RISE_TARGET),
        })

    cols = [
        "Name","Symbol","Green EMA%FS","Gold Dip","Blue Price Action",
        "Signal",
        "% Drop30","% Rise30","% Drop90","% Rise90","% Drop365","% Rise365"
    ]
    df = pd.DataFrame(rows, columns=cols)

    # sort by Green EMA%FS (None or NaN go to bottom)
    df = df.sort_values("Green EMA%FS", na_position="last", ascending=True)
    return df









# ---------------------------
# Phase 4: Orchestration
# ---------------------------

def run_ubc_pro():
    global DELETE_ASSETS_FILE

    script_dir      = os.path.dirname(os.path.abspath(__file__))
    asset_list_path = os.path.join(script_dir, "Asset List.txt")
    csv_file        = os.path.join(script_dir, "Assets_Data.csv")
    parquet_file    = os.path.join(script_dir, "Assets_Data.parquet")
    excel_file      = os.path.join(script_dir, "Assets_Processed.xlsx")
    charts_dir      = os.path.join(script_dir, "Charts")
    DELETE_ASSETS_FILE = os.path.join(script_dir, "Delete Assets.txt")

    # Phase 1: Download & merge historical data
    print("\n=== Phase 1: Download & merge historical data ===")
    combined = download_asset_data(
        asset_list_path,
        csv_file,
        parquet_file,
        DELETE_ASSETS_FILE
    )
    if combined.empty:
        print("No data to process. Exiting.")
        return

    # ── Now that today's fetch is done, apply delete-list cleanup ──────────
    if os.path.exists(DELETE_ASSETS_FILE):
        dels = set(pd.read_csv(DELETE_ASSETS_FILE)["Symbol"].str.strip())
        if os.path.exists(parquet_file):
            df_pq = pd.read_parquet(parquet_file)
            df_pq = df_pq[~df_pq["Symbol"].isin(dels)]
            df_pq.to_parquet(parquet_file, index=False)
            logging.info(f"Removed {len(dels)} symbols from Parquet per delete list.")
        elif os.path.exists(csv_file):
            df_csv = pd.read_csv(csv_file)
            df_csv = df_csv[~df_csv["Symbol"].isin(dels)]
            df_csv.to_csv(csv_file, index=False)
            logging.info(f"Removed {len(dels)} rows from CSV per delete list.")

    # … continue with Phase 2, Phase 3, etc. …


    # If targeting a single asset
    if TARGET_ASSET:
        df_target = combined[combined["Symbol"] == TARGET_ASSET].copy()
        if df_target.empty:
            print(f"{TARGET_ASSET} not found. Exiting.")
            return
        df_target = df_target.sort_values("Date").reset_index(drop=True)
        df_target = add_tracking_columns(df_target)
        df_target = compute_three_band_entries(df_target)
        cols = ["Date", "Low", "EMA", "EMA%FS", "Action", "entryColor"]
        print(df_target[cols].to_string(index=False))
        combined = df_target

    print(f"✅ Phase 1 complete: got {len(combined)} rows total.")

    # Phase 2: Compute per-asset signals
    groups = [g for _, g in combined.groupby("Symbol")]
    with ProcessPoolExecutor(max_workers=4) as exe:
        processed = list(exe.map(process_asset_group_wrapper, groups))
    processed = [g for g in processed if not g.empty]
    if not processed:
        print("No assets passed filters. Exiting.")
        return
    proc_df = pd.concat(processed, ignore_index=True)

    # ──────────────────────────────────────────────
    # Phase 3a: JSON export + 90%-hit-ratio filter + interpretations
    # ──────────────────────────────────────────────
    os.makedirs(charts_dir, exist_ok=True)
    interpretations: Dict[str, Dict[str, str]] = {}
    good_symbols = []

   # 1) hit-ratio screening — keep if any of the three rise-ratios ≥ 90%
    for sym, grp in proc_df.groupby("Symbol"):
        entries = grp[grp["entryColor"].notnull()]
        n = len(entries)
        if n == 0:
            continue

        pct_r30  = entries["MaxRise1Y"].ge(RISE_TARGET).sum()   / n
        pct_r90  = entries["MaxRise3Y"].ge(RISE_TARGET).sum()   / n
        pct_r365 = entries["MaxRise3Y"].ge(RISE_TARGET).sum()   / n

        if any(p >= 0.90 for p in (pct_r30, pct_r90, pct_r365)):
            good_symbols.append(sym)
            # print the actual ratios for this ticker
            print(
                f"Including {sym}: rise‐ratios → "
                f"30d={pct_r30:.0%}, 90d={pct_r90:.0%}, 1y={pct_r365:.0%}"
            )
        else:
            print(
                f"Skipping {sym}: rise‐ratios = "
                f"{pct_r30:.0%}, {pct_r90:.0%}, {pct_r365:.0%}"
            )





    # 2) export JSON + collect interpretations
    for sym, grp in proc_df.groupby("Symbol"):
        if sym not in good_symbols:
            continue
        clean = sym.replace(".", "_").upper()

        tmp = grp.copy().where(pd.notnull(grp), None)
        tmp = tmp.rename(columns={"Date":"date","Low":"low","High":"high","Close":"close"})
        records = tmp.to_dict(orient="records")

        mixed = []
        for i, rec in enumerate(records):
            if rec.get("entryColor") is None:
                core = {
                    "date":rec["date"],"low":rec["low"],
                    "high":rec["high"],"close":rec["close"],
                    "DipFromATH":rec.get("DipFromATH"),
                    "Baseline2":rec.get("Baseline2"),
                    "Baseline":rec.get("Baseline"),
                }
                if i >= EMA_INIT_INDEX:
                    core["EMA"] = rec.get("EMA")
                mixed.append(core)
            else:
                mixed.append(rec)

        with open(os.path.join(charts_dir, f"{clean}.json"), "w") as f:
            json.dump({"tracker": mixed}, f, indent=2)
            f.write("\n")

        report = build_report_dict(grp) or {}
        with open(os.path.join(charts_dir, f"{clean}_Report.json"), "w") as f:
            json.dump(report, f, indent=2)
            f.write("\n")

        interpretations[sym] = {
            **report.get("interpretation", {}),
            "currentBaseline":  report.get("currentBaseline", ""),
            "currentDipFromATH":report.get("currentDipFromATH", ""),
            "recentClose":      report.get("recentClose", None),
        }

    print(f"✅ Per-asset JSON written under {charts_dir}/")

     # ──────────────────────────────────────────────
    # Phase 3b: Build and write Excel Report sheet
    # ──────────────────────────────────────────────
    filtered_proc_df = proc_df[proc_df["Symbol"].isin(good_symbols)]
    rpt_df = create_report_sheet(filtered_proc_df, interpretations)

    with pd.ExcelWriter(excel_file, engine="xlsxwriter") as writer:
        # Report sheet
        rpt_df.to_excel(writer, sheet_name="Report", index=False)
        ws = writer.sheets["Report"]
        ws.freeze_panes(1, 0)

        # 0% format for all percent columns
        pct_fmt = writer.book.add_format({'num_format': '0%'})
        for col in [
            "Green EMA%FS", "Gold Dip",
            "% Drop30", "% Rise30", "% Drop90", "% Rise90", "% Drop365", "% Rise365"
        ]:
            idx = rpt_df.columns.get_loc(col)
            ws.set_column(idx, idx, None, pct_fmt)

        # wrap Signal text
        sig_fmt = writer.book.add_format({'text_wrap': True})
        sig_idx = rpt_df.columns.get_loc("Signal")
        ws.set_column(sig_idx, sig_idx, 50, sig_fmt)

        # per-asset detail sheets (only good_symbols)
        for sym, grp in filtered_proc_df.groupby("Symbol"):
            grp.to_excel(writer, sheet_name=sym, index=False)

    print(f"✅ Excel written to: {excel_file}")





if __name__ == "__main__":
    run_ubc_pro()
